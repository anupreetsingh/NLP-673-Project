{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Useful Links  \n",
        "A fine-tuning of the Idefics3-8B-Llama3 model with the ROCO dataset\n",
        "\n",
        "*   https://huggingface.co/eltorio/IDEFICS3_ROCO\n",
        "*   https://colab.research.google.com/#scrollTo=8F3w0kcbAMtC&fileId=https%3A//huggingface.co/eltorio/IDEFICS3_ROCO/blob/main/ROCO-idefics3.ipynb  \n",
        "\n",
        "\n",
        "Example notebook provided by the BioViL-T model  \n",
        "https://notebooks.gesis.org/binder/jupyter/user/microsoft-hi-ml-w1rabu8m/doc/tree/hi-ml-multimodal/notebooks/phrase_grounding.ipynb\n",
        "\n",
        "ROCO dataset  \n",
        "https://huggingface.co/datasets/eltorio/ROCO-radiology\n"
      ],
      "metadata": {
        "id": "wF68--hWcvnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip_source = \"hi-ml-multimodal\""
      ],
      "metadata": {
        "id": "HaE3tr5eGok0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install {pip_source}"
      ],
      "metadata": {
        "id": "rNmagSpaGoLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "J81lgio3IDbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from health_multimodal.common.visualization import plot_phrase_grounding_similarity_map\n",
        "from health_multimodal.text import get_bert_inference\n",
        "from health_multimodal.text.utils import BertEncoderType\n",
        "from health_multimodal.image import get_image_inference\n",
        "from health_multimodal.image.utils import ImageModelType\n",
        "from health_multimodal.vlp import ImageTextInferenceEngine"
      ],
      "metadata": {
        "id": "gHz0HqPpGjm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_inference = get_bert_inference(BertEncoderType.BIOVIL_T_BERT)\n",
        "image_inference = get_image_inference(ImageModelType.BIOVIL_T)"
      ],
      "metadata": {
        "id": "n4SRy801GtfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_text_inference = ImageTextInferenceEngine(\n",
        "    image_inference_engine=image_inference,\n",
        "    text_inference_engine=text_inference,\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_text_inference.to(device)"
      ],
      "metadata": {
        "id": "sBnG2VOKGwq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./output_directory\",\n",
        "  report_to = [], # default is \"all\" which is annoying\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size=8,\n",
        "  num_train_epochs=3,\n",
        "  save_steps = 10,\n",
        "  resume_from_checkpoint = True,\n",
        ")"
      ],
      "metadata": {
        "id": "4lxokjhG9JZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"eltorio/ROCO-radiology\")"
      ],
      "metadata": {
        "id": "O4sEdX0iHgxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(input):\n",
        "  \"\"\"Reshapes the dataset to contains only the embedded information for use in the model\n",
        "\n",
        "  Args:\n",
        "    input: A dictionary from the the dataset containing a 'image' and 'caption' keys for extracting embeddings\n",
        "\n",
        "  Returns:\n",
        "    A dictionary with the keys 'input' containing the image embedding, and 'labels' containing the text embedding\n",
        "  \"\"\"\n",
        "  image = input['image']\n",
        "  caption = input['caption']\n",
        "  image_path = Path(tempfile.tempdir, \"downloaded_chest_xray.jpg\")\n",
        "  image.save(image_path)\n",
        "  image_tokens = image_text_inference.image_inference_engine.get_projected_global_embedding(image_path=image_path)\n",
        "  text_tokens = image_text_inference.text_inference_engine.get_embeddings_from_prompt(input['caption'])\n",
        "  return {\n",
        "    \"input\": image_tokens,\n",
        "    \"labels\": text_tokens\n",
        "  }"
      ],
      "metadata": {
        "id": "iZtJk2B--tVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "# grabs the first 5 as to not load everything when testing code\n",
        "small_dataset = DatasetDict({\n",
        "    split: dataset.select(range(5)) for split, dataset in train_dataset.items()\n",
        "})"
      ],
      "metadata": {
        "id": "GIGeqg0LRZ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small = False # Set to True for testing, False for production\n",
        "used_dataset = small_dataset if small else train_dataset\n",
        "tokenized_dataset = used_dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "jyn0wLGKJXhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unused columns when training\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(train_dataset[\"test\"].column_names)"
      ],
      "metadata": {
        "id": "6QZz0yNu_nB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the training arguments for training the model\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./output_directory\",\n",
        "  report_to = [], # default is \"all\" which is annoying\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size=8,\n",
        "  num_train_epochs=3,\n",
        "  save_steps = 10,\n",
        "  resume_from_checkpoint = True,\n",
        ")"
      ],
      "metadata": {
        "id": "9ad6aYwP-vZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# wrapper model for training\n",
        "class FineTunedModel(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(FineTunedModel, self).__init__()\n",
        "        self.original_model = original_model\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input=None, labels=None):\n",
        "        return {\"loss\": self.criterion(torch.tensor(input, requires_grad=True), labels.squeeze(1)) }"
      ],
      "metadata": {
        "id": "E6dXDS9xabtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "trainable_model = FineTunedModel(image_text_inference)\n",
        "trainer = Trainer(\n",
        "  model=trainable_model,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_dataset[\"train\"],\n",
        "  eval_dataset=tokenized_dataset[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "bt8jMBd7_-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training takes a very long time\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ja3-4u5rZbc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImageEmbeddings(input):\n",
        "  \"\"\"Transforms an image to its embedded form using the model\n",
        "\n",
        "  Args:\n",
        "    input: A dictionary from the the dataset containing a 'image' key for extracting embeddings\n",
        "\n",
        "  Returns:\n",
        "    A tensor of the embeddings\n",
        "  \"\"\"\n",
        "  image = input['image']\n",
        "  image_path = Path(tempfile.tempdir, \"downloaded_chest_xray.jpg\")\n",
        "  image.save(image_path)\n",
        "  image_tokens = image_text_inference.image_inference_engine.get_projected_global_embedding(image_path=image_path)\n",
        "  return image_tokens"
      ],
      "metadata": {
        "id": "g8P0fTwWjEG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImageEmbeddingsFromImage(image_path):\n",
        "  \"\"\"Transforms an image to its embedded form using the model\n",
        "\n",
        "  Args:\n",
        "    input: A file path to an image for extracting embeddings\n",
        "\n",
        "  Returns:\n",
        "    A tensor of the embeddings\n",
        "  \"\"\"\n",
        "  path = Path(image_path)\n",
        "  return image_text_inference.image_inference_engine.get_projected_global_embedding(image_path=path)"
      ],
      "metadata": {
        "id": "1qcefRtpemQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def GetBestLabel(image_embeddings, labels):\n",
        "  \"\"\"Chooses the best label for a given image_embeddings\n",
        "\n",
        "  Args:\n",
        "    image_embeddings: the embeddings for a particular image of interest obtained from getImageEmbeddings\n",
        "    labels: the collection of labels that to choose the 'best' label from\n",
        "\n",
        "  Returns:\n",
        "    A text label for the particular image\n",
        "  \"\"\"\n",
        "  embeddings = image_embeddings\n",
        "  sim = []\n",
        "  for label in labels:\n",
        "    a = embeddings\n",
        "    b = torch.tensor(label[0])\n",
        "    s = F.cosine_similarity(a, b, dim=-1)\n",
        "    sim.append(s)\n",
        "  best = sim.index(max(sim))\n",
        "  final = train_dataset['train'][best]['caption']\n",
        "  return final\n",
        "\n"
      ],
      "metadata": {
        "id": "S-X1LsbznWv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoreLabels(caption, label):\n",
        "  \"\"\"Scores how well a given caption label pair is using cosine simularity\n",
        "\n",
        "  Args:\n",
        "    caption: A text based caption to score\n",
        "    label: The text based label to compare with caption\n",
        "\n",
        "  Returns:\n",
        "    A float for how well the input is correlated in the range (-1,1)\n",
        "  \"\"\"\n",
        "  return image_text_inference.text_inference_engine.get_pairwise_similarities(caption, label)"
      ],
      "metadata": {
        "id": "GNrUo-aMxSDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GetCaptionForImage(file_path):\n",
        "  \"\"\"Gets the caption for any arbitrary image using the model\n",
        "\n",
        "  Args:\n",
        "    file_path: the path to the image to caption\n",
        "\n",
        "  Returns:\n",
        "    A text based caption for the image\n",
        "  \"\"\"\n",
        "  labels = tokenized_dataset['train']['labels']\n",
        "  embeddings = getImageEmbeddingsFromImage(file_path)\n",
        "  label = GetBestLabel(embeddings, labels)\n",
        "  return label.strip()"
      ],
      "metadata": {
        "id": "7yQvzBkEh3FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs the evaluation on the model. It looks at its guessed label, its original caption, and its score between the two. A score closer to 1 is better."
      ],
      "metadata": {
        "id": "pGgakWMkpYm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "labels = tokenized_dataset['train']['labels']\n",
        "start = 0\n",
        "end = 5\n",
        "for i in range(start, end):\n",
        "  curr = train_dataset['validation'][i]\n",
        "  image = curr['image']\n",
        "  caption = curr['caption']\n",
        "  embeddings = getImageEmbeddings(curr)\n",
        "  label = GetBestLabel(embeddings, labels)\n",
        "  score = scoreLabels(caption, label)\n",
        "  print(f\"image: {image}\")\n",
        "  print(f\"caption: {caption.strip()}\")\n",
        "  print(f\"label: {label.strip()}\")\n",
        "  print(f\"score: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "ZpPorO7Nvt4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload a file and get a caption back using the model"
      ],
      "metadata": {
        "id": "cNefGP1xpOV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only works in google colab\n",
        "from google.colab import files\n",
        "file_name = 'uploaded_file.png'\n",
        "file_path = '/content/' + file_name\n",
        "uploaded = files.upload_file(file_name)\n",
        "\n",
        "caption = GetCaptionForImage(file_path)\n",
        "print(caption)"
      ],
      "metadata": {
        "id": "-pvSWvxZciCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}